// File: C:/Capstone/test/src/InterviewRoom.jsx

import React, { useEffect, useRef, useState } from "react";
import { evaluateSelfIntroAPI } from "./apis/evaluateSelfIntro";
import { generateQuestionsAPI } from "./apis/question";
import { evaluateAnswerAPI } from "./apis/evaluateAnswer";
import { generateCompanyPhilosophyQuestionAPI } from "./apis/philosophyQuestion";

import { FaceMesh } from "@mediapipe/face_mesh";
import { Camera } from "@mediapipe/camera_utils";

import "./components/InterviewRoom.css";

const InterviewRoom = () => {
  const userVideoRef   = useRef(null);
  const recognitionRef = useRef(null);
  const nodCountRef    = useRef(0);
  const lastYRef       = useRef(null);

  const [nods, setNods]               = useState(0);
  const [questionIndex, setQuestionIndex] = useState(0);
  const [questions, setQuestions]     = useState([
    { text: "1ë¶„ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.", type: "selfIntro" }
  ]);
  const [finalAnswerText, setFinalAnswerText] = useState("");
  const [sttLoading, setSttLoading]           = useState(false);
  const [aiImage, setAiImage]                 = useState("50.png");
  const [timeLeft, setTimeLeft]               = useState(90);
  const [timerRunning, setTimerRunning]       = useState(false);
  const [showWarning, setShowWarning]         = useState(false);

  const [field] = useState(localStorage.getItem("selectedField") || "í”„ë¡ íŠ¸ ê°œë°œ");
  const [role]  = useState(localStorage.getItem("selectedRole") || "frontend");

  const WARNING_THRESHOLD = 10;

  useEffect(() => {
    navigator.mediaDevices.getUserMedia({ video: true })
      .then(stream => {
        const video = userVideoRef.current;
        video.srcObject = stream;
        video.play().catch(() => {});
      })
      .catch(err => console.error("ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨:", err));
  }, []);

  useEffect(() => {
    if (!userVideoRef.current) return;

    const faceMesh = new FaceMesh({
      locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`
    });
    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: false,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    const NOD_THRESHOLD = 0.008;

    faceMesh.onResults(results => {
      const lm = results.multiFaceLandmarks?.[0];
      if (!lm) return;
      const y = lm[1].y;
      if (lastYRef.current !== null) {
        const dy = lastYRef.current - y;
        if (dy > NOD_THRESHOLD) {
          nodCountRef.current += 1;
          setNods(nodCountRef.current);
          if (nodCountRef.current > WARNING_THRESHOLD) {
            setShowWarning(true);
            setTimeout(() => setShowWarning(false), 3000);
          }
        }
      }
      lastYRef.current = y;
    });

    const camera = new Camera(userVideoRef.current, {
      onFrame: async () => await faceMesh.send({ image: userVideoRef.current }),
      width: 640,
      height: 480
    });
    camera.start();

    return () => {
      camera.stop();
      faceMesh.close();
    };
  }, []);

  useEffect(() => {
    if (!timerRunning) return;
    const t = setInterval(() => {
      setTimeLeft(prev => {
        if (prev <= 1) {
          clearInterval(t);
          handleNextQuestion();
          return 0;
        }
        return prev - 1;
      });
    }, 1000);
    return () => clearInterval(t);
  }, [timerRunning]);

  const startSTT = () => {
    if (questionIndex >= questions.length) return;
    setFinalAnswerText("");
    setTimerRunning(true);

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("ë¸Œë¼ìš°ì €ê°€ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•Šì•„ìš” ğŸ˜¢");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.lang = "ko-KR";
    recognition.interimResults = true;
    recognition.continuous = true;

    recognition.onstart = () => console.log("ğŸ™ï¸ ìŒì„± ì¸ì‹ ì‹œì‘");
    recognition.onresult = event => {
      const transcript = event.results[event.results.length - 1][0].transcript;
      setFinalAnswerText(prev => prev + " " + transcript);
    };
    recognition.onerror = e => console.error("ìŒì„± ì¸ì‹ ì˜¤ë¥˜:", e.error);
    recognition.onend = () => console.log("ğŸ›‘ ìŒì„± ì¸ì‹ ì¤‘ë‹¨");

    recognition.start();
    recognitionRef.current = recognition;
  };

  const stopSTT = () => recognitionRef.current?.stop();

  const updateAiImageByScore = score => {
    console.log("ğŸ­ ì´ë¯¸ì§€ ë³€ê²½ì„ ìœ„í•œ ì ìˆ˜:", score);
    if (score >= 80) setAiImage("80j.gif");
    else if (score >= 30) setAiImage("30j.gif");
    else setAiImage("20j.gif");
  };

  const handleSelfIntroEvaluation = async answer => {
    setSttLoading(true);
    try {
      console.log("ğŸ“ ìê¸°ì†Œê°œ í‰ê°€ ì‹œì‘:", answer);
      const { totalScore, followUpQuestions } = await evaluateSelfIntroAPI(answer);
      console.log("ğŸ“Š ìê¸°ì†Œê°œ ì´ì :", totalScore);
      console.log("ğŸ“Œ í›„ì† ì§ˆë¬¸:", followUpQuestions);
      updateAiImageByScore(totalScore);

      const rawPhilosophy = await generateCompanyPhilosophyQuestionAPI();
      const philosophyQ = {
        text: rawPhilosophy?.question?.trim() ||
          "ìš°ë¦¬ íšŒì‚¬ì˜ ì² í•™ì´ë‚˜ ì¡°ì§ ë¬¸í™”ë¥¼ ì–´ë–»ê²Œ ë°”ë¼ë³´ì‹œë‚˜ìš”?",
        type: "philosophy"
      };

      let newQs = [questions[0]];
      if (totalScore < 60) {
        const jqs = await generateQuestionsAPI({ role, field, count: 4 });
        newQs = [...newQs, ...jqs.map(q => ({ text: q, type: role })), philosophyQ];
      } else if (followUpQuestions.length) {
        const jqs = await generateQuestionsAPI({ role, field, count: 2 });
        newQs = [
          ...newQs,
          ...followUpQuestions.map(q => ({ text: q, type: role })),
          ...jqs.map(q => ({ text: q, type: role })),
          philosophyQ
        ];
      } else {
        const jqs = await generateQuestionsAPI({ role, field, count: 4 });
        newQs = [...newQs, ...jqs.map(q => ({ text: q, type: role })), philosophyQ];
      }
      setQuestions(newQs);
      setQuestionIndex(1);
    } catch (e) {
      console.error("âŒ ìê¸°ì†Œê°œ í‰ê°€ ì‹¤íŒ¨:", e);
    } finally {
      setSttLoading(false);
    }
  };

  const handleAnswerEvaluation = async answer => {
    if (!answer.trim()) return;
    setSttLoading(true);
    try {
      console.log(`ğŸ“ ì§ˆë¬¸ ${questionIndex + 1}ë²ˆ ë‹µë³€ í‰ê°€ ì‹œì‘`);
      console.log("âœ… í‰ê°€í•  ë‹µë³€:", answer);
      const cq = questions[questionIndex];
      const roleToUse = cq.type === "philosophy" ? "philosophy" : role;
      const res = await evaluateAnswerAPI({ role: roleToUse, answerText: answer });
      const m = res.replace(/\n/g, " ").match(/ì´ì :\s*(\d+)/);
      const score = m ? parseInt(m[1], 10) : 0;
      console.log(`ğŸ“Š ì§ˆë¬¸ ${questionIndex + 1}ë²ˆ ì ìˆ˜:`, score);
      updateAiImageByScore(score);
    } catch (e) {
      console.error("âŒ ë‹µë³€ í‰ê°€ ì‹¤íŒ¨:", e);
    } finally {
      setSttLoading(false);
    }
  };

  const handleNextQuestion = async () => {
    stopSTT();
    setTimerRunning(false);
    setTimeLeft(90);

    if (questionIndex === 0) {
      await handleSelfIntroEvaluation(finalAnswerText);
    } else {
      await handleAnswerEvaluation(finalAnswerText);
      if (questionIndex >= questions.length - 1) {
        console.log("ğŸ‰ ëª¨ë“  ì§ˆë¬¸ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!");
        setQuestionIndex(questions.length);
      } else {
        setQuestionIndex(q => q + 1);
      }
    }
    setFinalAnswerText("");
  };

  return (
    <div className="interview-room">
      {showWarning && <div className="nod-warning">âš ï¸ ì£¼ì˜: ê³¼ë„í•œ ë„ë•ì„ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.</div>}
      <div className="ai-section">
        <div className="avatar-wrapper">
          <img src={`/${aiImage}`} alt="AI ë©´ì ‘ê´€" className="avatar-gif" />
        </div>
        <h3>AI ë©´ì ‘ê´€</h3>
        <p className="question">
          {questionIndex < questions.length
            ? `ì§ˆë¬¸ ${questionIndex + 1}. ${questions[questionIndex].text}`
            : "ëª¨ë“  ì§ˆë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰"}
        </p>
      </div>

      <div className="user-section">
        <video
          ref={userVideoRef}
          autoPlay
          playsInline
          muted
          className="user-video"
        />
        <h3>ì§€ì›ì</h3>
      </div>
      
      <div
        className="timer-box"
        style={{ "--timer-progress": `${(timeLeft / 90) * 100}%` }}
      >
        <div className="timer-text1">
          {Math.floor(timeLeft / 60)}:{String(timeLeft % 60).padStart(2, "0")}
        </div>
      </div>




    

      <div className="timer-circle">
        <svg className="progress-ring">
          <circle
            className="progress-ring__circle"
            stroke="white"
            strokeWidth="8"
            fill="transparent"
            r="54"
            cx="60"
            cy="60"
            style={{
              strokeDasharray: 339.292,
              strokeDashoffset: (1 - timeLeft / 90) * 339.292,
              transition: "stroke-dashoffset 1s linear",
            }}
          />
        </svg>
        <div className="timer-text">
          {Math.floor(timeLeft / 60)}:{String(timeLeft % 60).padStart(2, "0")}
        </div>
        
        
      </div>
        

      




      <div className="stt-button-row">
        <button
          className="voice-button"
          onClick={startSTT}
          disabled={sttLoading || timerRunning || questionIndex >= questions.length}
        >
          {sttLoading ? "GPT í‰ê°€ ì¤‘..." : "ğŸ™ï¸ ë‹µë³€ ì‹œì‘"}
        </button>
        <button
          className="voice-button"
          onClick={handleNextQuestion}
          disabled={!timerRunning && questionIndex !== 0}
        >
          ë‹µë³€ ì™„ë£Œ
        </button>
      </div>
    </div>
  );
};

export default InterviewRoom;
